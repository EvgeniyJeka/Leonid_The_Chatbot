from llama_cpp import Llama
import logging
import os

logging.basicConfig(level=logging.INFO)

MAX_TOKENS_TOTAL_CONVERSATION = 15000
MAX_TOKENS_TO_GENERATE = 100

# TO DO:
# 1. Model path - to env. variable, take it from there (*)
# 2. An option to reset context (API request?) (*)
# 3. An option to dump conversation into a file
# 4. Context cut (automatically) - find a good point for this.
# 5. An option to set context for a new conversation via API request
# 6. Consider to add the name of the person who is making the conversation to constructor and
#    add it to the initial context

TIME_TO_THINK_RESPONSE = "Please wait a few moments - I need time to think on my next brilliant answer"
MODEL_PATH = os.getenv("MODEL_PATH")
MAX_CONTENT_LENGTH = os.getenv("MAX_CONTEXT_LENGHT")


class ChatBot:
    """
    This class uses  llama-cpp-python methods to load a model, send user prompts to the model
    and return the output generated by the model.
    """

    # context = "Your name is Leonid. Lisa is a developer she is debugging code, do your best to assist her. " \
    #           "You have plenty coins and stones near you."

    initial_context = "Your name is Leonid. "
    context = initial_context

    llm = None
    conversation_pause_flag = False

    def __init__(self):
        logging.info("Loading the model..")
        self.llm = Llama(model_path=MODEL_PATH, n_ctx=MAX_TOKENS_TOTAL_CONVERSATION)
        logging.info("Model loaded.")

    def inject_context(self, injected_context: str):
        self.context = injected_context
        return True

    def cut_context(self, text):
        """
        This method comes to handle a case in which the conversation context (all previous questions and answers)
        becomes too long. It can reduce performance and significantly increase the time it takes for the model
        to deal with the next input.
        Once the total length of the conversation reaches the limit (which is set in env. variable)
        it is cut by 2 (the closest pair to the cutting point "Question - Answer" is taken).
        :param text: str
        :return: str
        """

        logging.info(f"Chat Bot: The context length has exceeded the limit, it has reached {len(text)}\n"
                     f"Chat Bot: Cutting the context by half.")

        list_converted = text.split("Question")
        list_middle = int(len(list_converted) / 2)
        modified_list = list_converted[list_middle:]

        result = "Question" + "Question".join(modified_list)
        return result

    def send_prompt(self, user_input):
        """
        The method sends user prompt to the model and returns the output generated by the model.
        Every question (user prompt) and every answer (AI response) are added to the context (implicitly) to
        keep the conversation consistent. The context is attached to each user prompt.
        :param user_input: str
        :return: str
        """

        # Waiting for a response from Vicuna. Blocking additional input to prevent crash
        if self.conversation_pause_flag is True:
            return TIME_TO_THINK_RESPONSE

        self.conversation_pause_flag = True

        output = self.llm(f"Context: ({self.context}). Question: {user_input} Answer:",
                          max_tokens=MAX_TOKENS_TO_GENERATE,
                          stop=["\n", "Question:", "Q:"],
                          echo=True)

        text_line = output['choices'][0]['text']
        logging.info(f"Context: {text_line}")

        text_line_list = text_line.split("Answer:")
        response_returned = text_line_list[-1]

        adding_to_conversation_context = f" Question: {user_input}, Answer: {response_returned}"
        self.context = self.context + adding_to_conversation_context

        # Cutting conversation context to improve performance if it exceeds the limits
        if len(self.context) > int(MAX_CONTENT_LENGTH):
            modified_content = self.cut_context(self.context)
            self.context = self.initial_context + modified_content

        self.conversation_pause_flag = False

        return response_returned


# if __name__ == "__main__":
#     bot = ChatBot()
#     print(bot.send_prompt("Hello. How are you?"))
#     print(bot.send_prompt("Imagine you have 3 coins in your hand"))
#     print(bot.send_prompt("Add 2 more coins"))
#     print(bot.send_prompt("How many coins are now in your hand?"))



